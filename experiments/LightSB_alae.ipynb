{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea332904",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8c2dfa-90a6-479c-be2f-11d0e098affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deeplake in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (4.1.19)\n",
      "Requirement already satisfied: numpy in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from deeplake) (1.24.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dd478e-5628-4ac0-9856-3c09f61c542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yacs in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (0.1.8)\n",
      "Requirement already satisfied: PyYAML in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from yacs) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3c6e75-6985-464b-8ec1-a503529429a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geotorch in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=1.9 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from geotorch) (2.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.0.106)\n",
      "Requirement already satisfied: filelock in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (3.0.0)\n",
      "Requirement already satisfied: sympy in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch>=1.9->geotorch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->geotorch) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from jinja2->torch>=1.9->geotorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from sympy->torch>=1.9->geotorch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install geotorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbad85a-f2f2-47f7-8146-3af1bb159627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172212e0-b4c5-4082-b9fe-67bd03a564a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ee257e-ca2d-40f7-aa8b-27d9c69fa33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: scikit-learn in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: fsspec in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy numpy matplotlib scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88aa5c-ef8c-497b-82b9-84b7061eba9d",
   "metadata": {},
   "source": [
    "## Using LIGHTSB creators FFHQ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd716680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../models/LightSB/ALAE\")\n",
    "sys.path.append(\"../models/LightSB\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.light_sb import LightSB\n",
    "from src.distributions import LoaderSampler, TensorSampler\n",
    "import deeplake\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from alae_ffhq_inference import load_model, encode, decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e65eb",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13fd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 512\n",
    "assert DIM > 1\n",
    "\n",
    "INPUT_DATA = \"ADULT\" # MAN, WOMAN, ADULT, CHILDREN\n",
    "TARGET_DATA = \"CHILDREN\" # MAN, WOMAN, ADULT, CHILDREN\n",
    "\n",
    "OUTPUT_SEED = 0xBADBEEF\n",
    "BATCH_SIZE = 128\n",
    "EPSILON = 0.1\n",
    "D_LR = 1e-3 # 1e-3 for eps 0.1\n",
    "D_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "N_POTENTIALS = 10\n",
    "SAMPLING_BATCH_SIZE = 128\n",
    "INIT_BY_SAMPLES = True\n",
    "IS_DIAGONAL = True\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "CONTINUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d489a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f'LightSB_ALAE_{INPUT_DATA}_TO_{TARGET_DATA}_EPSILON_{EPSILON}'\n",
    "OUTPUT_PATH = '../checkpoints/{}'.format(EXP_NAME)\n",
    "\n",
    "config = dict(\n",
    "    DIM=DIM,\n",
    "    D_LR=D_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    EPSILON=EPSILON,\n",
    "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
    "    N_POTENTIALS=N_POTENTIALS,\n",
    "    INIT_BY_SAMPLES=INIT_BY_SAMPLES,\n",
    "    IS_DIAGONAL=IS_DIAGONAL,\n",
    ")\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcec61",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2ad09",
   "metadata": {},
   "source": [
    "### TO DOWNLOAD PRE-PROCESSED ALAE DATA, UNCOMMENT THE CODE OF THE NEXT CELL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c89c2e9-e66a-4fe2-a708-efff7f5d4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (5.2.0)\n",
      "Requirement already satisfied: tqdm in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: requests[socks] in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from beautifulsoup4->gdown) (4.13.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/andrey/projects/venvs/jypyter_venv3_8/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4bd56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Vi6NzxCsS23GBNq48E-97Z9UuIuNaxPJ\n",
      "To: /home/andrey/projects/LightSB/data/age.npy\n",
      "100%|██████████| 560k/560k [00:00<00:00, 4.08MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1SEdsmQGL3mOok1CPTBEfc_O1750fGRtf\n",
      "To: /home/andrey/projects/LightSB/data/gender.npy\n",
      "100%|██████████| 1.68M/1.68M [00:00<00:00, 8.82MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ENhiTRsHtSjIjoRu1xYprcpNd8M9aVu8\n",
      "From (redirected): https://drive.google.com/uc?id=1ENhiTRsHtSjIjoRu1xYprcpNd8M9aVu8&confirm=t&uuid=380f23b9-8f26-4bd8-8882-6a8db3f39b85\n",
      "To: /home/andrey/projects/LightSB/data/latents.npy\n",
      "100%|██████████| 143M/143M [00:02<00:00, 69.0MB/s] \n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1SjBWWlPjq-dxX4kxzW-Zn3iUR3po8Z0i\n",
      "From (redirected): https://drive.google.com/uc?id=1SjBWWlPjq-dxX4kxzW-Zn3iUR3po8Z0i&confirm=t&uuid=ebbec06b-05d4-4200-af8a-0b157df6ddc2\n",
      "To: /home/andrey/projects/LightSB/data/test_images.npy\n",
      "100%|██████████| 944M/944M [00:13<00:00, 71.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "urls = {\n",
    "    \"../data/age.npy\": \"https://drive.google.com/uc?id=1Vi6NzxCsS23GBNq48E-97Z9UuIuNaxPJ\",\n",
    "    \"../data/gender.npy\": \"https://drive.google.com/uc?id=1SEdsmQGL3mOok1CPTBEfc_O1750fGRtf\",\n",
    "    \"../data/latents.npy\": \"https://drive.google.com/uc?id=1ENhiTRsHtSjIjoRu1xYprcpNd8M9aVu8\",\n",
    "    \"../data/test_images.npy\": \"https://drive.google.com/uc?id=1SjBWWlPjq-dxX4kxzW-Zn3iUR3po8Z0i\",\n",
    "}\n",
    "\n",
    "for name, url in urls.items():\n",
    "    gdown.download(url, os.path.join(f\"{name}\"), quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3af9360e-6e0b-488e-ab21-8f179aa40d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = np.load(\"../data/latents.npy\")\n",
    "gender = np.load(\"../data/gender.npy\")\n",
    "age = np.load(\"../data/age.npy\")\n",
    "test_inp_images = np.load(\"../data/test_images.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84d41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# To download data use\n",
    "\n",
    "train_size = 60000\n",
    "test_size = 10000\n",
    "\n",
    "latents = np.load(\"../data/latents.npy\")\n",
    "gender = np.load(\"../data/gender.npy\")\n",
    "age = np.load(\"../data/age.npy\")\n",
    "test_inp_images = np.load(\"../data/test_images.npy\")\n",
    "\n",
    "train_latents, test_latents = latents[:train_size], latents[train_size:]\n",
    "train_gender, test_gender = gender[:train_size], gender[train_size:]\n",
    "train_age, test_age = age[:train_size], age[train_size:]\n",
    "\n",
    "if INPUT_DATA == \"MAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif INPUT_DATA == \"WOMAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif INPUT_DATA == \"ADULT\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif INPUT_DATA == \"CHILDREN\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "x_data_train = train_latents[x_inds_train]\n",
    "x_data_test = test_latents[x_inds_test]\n",
    "\n",
    "if TARGET_DATA == \"MAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif TARGET_DATA == \"WOMAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif TARGET_DATA == \"ADULT\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif TARGET_DATA == \"CHILDREN\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "y_data_train = train_latents[y_inds_train]\n",
    "y_data_test = test_latents[y_inds_test]\n",
    "\n",
    "X_train = torch.tensor(x_data_train)\n",
    "Y_train = torch.tensor(y_data_train)\n",
    "\n",
    "X_test = torch.tensor(x_data_test)\n",
    "Y_test = torch.tensor(y_data_test)\n",
    "\n",
    "X_sampler = TensorSampler(X_train, device=\"cpu\")\n",
    "Y_sampler = TensorSampler(Y_train, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c22df8",
   "metadata": {},
   "source": [
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fce3c8",
   "metadata": {},
   "source": [
    "## LightSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e304d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(OUTPUT_SEED); np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "D = LightSB(dim=DIM, n_potentials=N_POTENTIALS, epsilon=EPSILON,\n",
    "            sampling_batch_size=SAMPLING_BATCH_SIZE, S_diagonal_init=0.1,\n",
    "            is_diagonal=IS_DIAGONAL).cpu()\n",
    "\n",
    "if INIT_BY_SAMPLES:\n",
    "    D.init_r_by_samples(Y_sampler.sample(N_POTENTIALS))\n",
    "    \n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b5901",
   "metadata": {},
   "source": [
    "## ALAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b7a9361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/projects/LightSB/notebooks/../ALAE/checkpointer.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# To download the required model run, run training_artifacts/download_all.py in the ALAE folder.\n",
    "\n",
    "model = load_model(\"../ALAE/configs/ffhq.yaml\", training_artifacts_dir=\"../ALAE/training_artifacts/ffhq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "442c2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:17<00:00, 567.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#wandb.init(name=EXP_NAME, config=config)\n",
    "\n",
    "for step in tqdm(range(CONTINUE + 1, MAX_STEPS)):\n",
    "    D_opt.zero_grad()\n",
    "    \n",
    "    X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "    \n",
    "    log_potential = D.get_log_potential(X1)\n",
    "    log_C = D.get_log_C(X0)\n",
    "    \n",
    "    D_loss = (-log_potential + log_C).mean()\n",
    "    D_loss.backward()\n",
    "    D_gradient_norm = torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=D_GRADIENT_MAX_NORM)\n",
    "    D_opt.step()\n",
    "    \n",
    "    #wandb.log({f'D gradient norm' : D_gradient_norm.item()}, step=step)\n",
    "    #wandb.log({f'D_loss' : D_loss.item()}, step=step)\n",
    "         \n",
    "torch.save(D.state_dict(), os.path.join(OUTPUT_PATH, f'D.pt'))\n",
    "torch.save(D_opt.state_dict(), os.path.join(OUTPUT_PATH, f'D_opt.pt'))\n",
    "\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14887a",
   "metadata": {},
   "source": [
    "# Results plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "339e5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(OUTPUT_SEED); np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "inds_to_map = np.random.choice(np.arange((x_inds_test < 300).sum()), size=10, replace=False)\n",
    "number_of_samples = 3\n",
    "\n",
    "mapped_all = []\n",
    "latent_to_map = torch.tensor(test_latents[x_inds_test[inds_to_map]])\n",
    "\n",
    "inp_images = test_inp_images[x_inds_test[inds_to_map]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        mapped = D(latent_to_map.cpu())\n",
    "        mapped_all.append(mapped)\n",
    "    \n",
    "mapped = torch.stack(mapped_all, dim=1)\n",
    "\n",
    "decoded_all = []\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        decoded_img = decode(model, mapped[:, k])\n",
    "        decoded_img = ((decoded_img * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).permute(0, 2, 3, 1).numpy()\n",
    "        decoded_all.append(decoded_img)\n",
    "        \n",
    "decoded_all = np.stack(decoded_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b94ec3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, number_of_samples+1, figsize=(number_of_samples+1, 10), dpi=200)\n",
    "\n",
    "for i, ind in enumerate(range(10)):\n",
    "    ax = axes[i]\n",
    "    ax[0].imshow(inp_images[ind])\n",
    "    for k in range(number_of_samples):\n",
    "        ax[k+1].imshow(decoded_all[ind, k])\n",
    "        \n",
    "        ax[k+1].get_xaxis().set_visible(False)\n",
    "        ax[k+1].set_yticks([])\n",
    "        \n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0983eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"ffhq_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40761169-0ef0-494d-81cf-e30a824a77a8",
   "metadata": {},
   "source": [
    "# On utk_face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47c9e932-e7e8-4105-bca1-2e97116e6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def upscale_image(image, target_size=(1024, 1024)):\n",
    "    \n",
    "    image_resized = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return image_resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a3a95-4668-4b91-993c-6aefcc536c98",
   "metadata": {},
   "source": [
    "### Get latents for utk_face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f42501f-a48e-4aa7-8a75-5af23c92a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "genders = []\n",
    "latents = []\n",
    "dataset_path = '../data/utk_face/crop_part1/'\n",
    "model = load_model(\"../ALAE/configs/ffhq.yaml\", training_artifacts_dir=\"../ALAE/training_artifacts/ffhq/\")\n",
    "\n",
    "for i in os.listdir(dataset_path):\n",
    "    split = i.split('_')\n",
    "    ages.append(int(split[0]))\n",
    "    genders.append(int(split[1]))\n",
    "    image_file = Image.open(dataset_path + i)\n",
    "    image = np.asarray(upscale_image(image_file))\n",
    "    \n",
    "    if image.shape[2] == 4:\n",
    "        image= image[:, :, :3]\n",
    "    image = image.transpose((2, 0, 1))\n",
    "\n",
    "    x = (\n",
    "            torch.tensor(\n",
    "                np.asarray(image, dtype=np.float32), device=\"cpu\", requires_grad=False\n",
    "            )\n",
    "            / 127.5\n",
    "            - 1.0\n",
    "        )\n",
    "    image_latents = model.encode(x[None, ...],8,1)[0].squeeze()\n",
    "    latents.append(image_latents)\n",
    "    image_file.close()\n",
    "latents_tensor = torch.stack(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "406bb2af-f5ff-49d2-a0dd-73e8b315ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_latents_tensor = latents_tensor.detach().cpu().numpy()\n",
    "np_ages = np.array(ages)\n",
    "np_genders = np.array(genders)\n",
    "np.save('utk_faces_latents_tensor.npy', np_latents_tensor)\n",
    "np.save('utk_faces_ages.npy', np_ages)\n",
    "np.save('utk_faces_genders.npy', np_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e4ea24-de69-49ee-8b47-4c5ead280cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = np.load('utk_faces_latents_tensor.npy')\n",
    "age = np.load('utk_faces_ages.npy')\n",
    "gender = np.load('utk_faces_genders.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797fb61b-134e-445f-87a2-8a65633025cf",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2968df64-9024-4bee-8ac3-e6dc5afd746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 512\n",
    "assert DIM > 1\n",
    "\n",
    "INPUT_DATA = \"CHILDREN\" # MAN, WOMAN, ADULT, CHILDREN\n",
    "TARGET_DATA = \"ADULT\" # MAN, WOMAN, ADULT, CHILDREN\n",
    "\n",
    "OUTPUT_SEED = 0xBADBEEF\n",
    "BATCH_SIZE = 128\n",
    "EPSILON = 0.1\n",
    "D_LR = 1e-3 # 1e-3 for eps 0.1\n",
    "D_GRADIENT_MAX_NORM = float(\"inf\")\n",
    "N_POTENTIALS = 10\n",
    "SAMPLING_BATCH_SIZE = 128\n",
    "INIT_BY_SAMPLES = True\n",
    "IS_DIAGONAL = True\n",
    "\n",
    "MAX_STEPS = 10000\n",
    "CONTINUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ef381cfe-ee10-49de-a8ae-47d4f51b31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = f'LightSB_ALAE_{INPUT_DATA}_TO_{TARGET_DATA}_EPSILON_{EPSILON}'\n",
    "OUTPUT_PATH = '../checkpoints/{}'.format(EXP_NAME)\n",
    "\n",
    "config = dict(\n",
    "    DIM=DIM,\n",
    "    D_LR=D_LR,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    EPSILON=EPSILON,\n",
    "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
    "    N_POTENTIALS=N_POTENTIALS,\n",
    "    INIT_BY_SAMPLES=INIT_BY_SAMPLES,\n",
    "    IS_DIAGONAL=IS_DIAGONAL,\n",
    ")\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32d9cb90-97bf-4f2c-8d89-f599dd887d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9780, 512)\n"
     ]
    }
   ],
   "source": [
    "print(latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9aec90e0-d223-4df7-8bbd-c24abe75c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download data use\n",
    "\n",
    "train_size = 9680\n",
    "test_size = 100\n",
    "\n",
    "# latents = np.load(\"../data/latents.npy\")\n",
    "# gender = np.load(\"../data/gender.npy\")\n",
    "# age = np.load(\"../data/age.npy\")\n",
    "#test_inp_images = np.load(\"../data/test_images.npy\")\n",
    "\n",
    "train_latents, test_latents = latents[:train_size], latents[train_size:]\n",
    "train_gender, test_gender = gender[:train_size], gender[train_size:]\n",
    "train_age, test_age = age[:train_size], age[train_size:]\n",
    "\n",
    "if INPUT_DATA == \"MAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif INPUT_DATA == \"WOMAN\":\n",
    "    x_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    x_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif INPUT_DATA == \"ADULT\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif INPUT_DATA == \"CHILDREN\":\n",
    "    x_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    x_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "x_data_train = train_latents[x_inds_train]\n",
    "x_data_test = test_latents[x_inds_test]\n",
    "\n",
    "if TARGET_DATA == \"MAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"male\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"male\").reshape(-1)]\n",
    "elif TARGET_DATA == \"WOMAN\":\n",
    "    y_inds_train = np.arange(train_size)[(train_gender == \"female\").reshape(-1)]\n",
    "    y_inds_test = np.arange(test_size)[(test_gender == \"female\").reshape(-1)]\n",
    "elif TARGET_DATA == \"ADULT\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age >= 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age >= 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "elif TARGET_DATA == \"CHILDREN\":\n",
    "    y_inds_train = np.arange(train_size)[\n",
    "        (train_age < 18).reshape(-1)*(train_age != -1).reshape(-1)\n",
    "    ]\n",
    "    y_inds_test = np.arange(test_size)[\n",
    "        (test_age < 18).reshape(-1)*(test_age != -1).reshape(-1)\n",
    "    ]\n",
    "y_data_train = train_latents[y_inds_train]\n",
    "y_data_test = test_latents[y_inds_test]\n",
    "\n",
    "X_train = torch.tensor(x_data_train)\n",
    "Y_train = torch.tensor(y_data_train)\n",
    "\n",
    "X_test = torch.tensor(x_data_test)\n",
    "Y_test = torch.tensor(y_data_test)\n",
    "\n",
    "X_sampler = TensorSampler(X_train, device=\"cpu\")\n",
    "Y_sampler = TensorSampler(Y_train, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093cee4-2210-4226-99a5-040f968ce8aa",
   "metadata": {},
   "source": [
    "# Model initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce974e8-9c6e-4808-8a1f-af49934981f3",
   "metadata": {},
   "source": [
    "## LightSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f84c9388-35c5-4554-9c9f-fab91e1ad35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(OUTPUT_SEED); np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "D = LightSB(dim=DIM, n_potentials=N_POTENTIALS, epsilon=EPSILON,\n",
    "            sampling_batch_size=SAMPLING_BATCH_SIZE, S_diagonal_init=0.1,\n",
    "            is_diagonal=IS_DIAGONAL).cpu()\n",
    "\n",
    "if INIT_BY_SAMPLES:\n",
    "    D.init_r_by_samples(Y_sampler.sample(N_POTENTIALS))\n",
    "    \n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=D_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96deaaf0-dec7-4305-9d8f-c6ab43fc6e4f",
   "metadata": {},
   "source": [
    "## ALAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66ddb12a-ee34-4eb4-8ec2-9bd45172241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the required model run, run training_artifacts/download_all.py in the ALAE folder.\n",
    "\n",
    "model = load_model(\"../ALAE/configs/ffhq.yaml\", training_artifacts_dir=\"../ALAE/training_artifacts/ffhq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "983f9040-c0b9-4243-b6c4-eda8eef44f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:09<00:00, 773.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#wandb.init(name=EXP_NAME, config=config)\n",
    "\n",
    "for step in tqdm(range(CONTINUE + 1, MAX_STEPS)):\n",
    "    D_opt.zero_grad()\n",
    "    \n",
    "    X0, X1 = X_sampler.sample(BATCH_SIZE), Y_sampler.sample(BATCH_SIZE)\n",
    "    \n",
    "    log_potential = D.get_log_potential(X1)\n",
    "    log_C = D.get_log_C(X0)\n",
    "    \n",
    "    D_loss = (-log_potential + log_C).mean()\n",
    "    D_loss.backward()\n",
    "    D_gradient_norm = torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=D_GRADIENT_MAX_NORM)\n",
    "    D_opt.step()\n",
    "    \n",
    "    #wandb.log({f'D gradient norm' : D_gradient_norm.item()}, step=step)\n",
    "    #wandb.log({f'D_loss' : D_loss.item()}, step=step)\n",
    "         \n",
    "torch.save(D.state_dict(), os.path.join(OUTPUT_PATH, f'D.pt'))\n",
    "torch.save(D_opt.state_dict(), os.path.join(OUTPUT_PATH, f'D_opt.pt'))\n",
    "\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb5cb762-11c9-4121-8d10-205711211a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = '../data/utk_face/crop_part1/'\n",
    "model = load_model(\"../ALAE/configs/ffhq.yaml\", training_artifacts_dir=\"../ALAE/training_artifacts/ffhq/\")\n",
    "test_images = []\n",
    "\n",
    "for i in os.listdir(dataset_path)[:-100:-1]:\n",
    "    image_file = Image.open(dataset_path + i)\n",
    "    image = np.asarray(upscale_image(image_file))\n",
    "    \n",
    "    # if image.shape[2] == 4:\n",
    "    #     image= image[:, :, :3]\n",
    "    # image = image.transpose((2, 0, 1))\n",
    "    test_images.append(image)\n",
    "    image_file.close()\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "340e7ce9-4885-4569-aa09-bd753ac0c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(OUTPUT_SEED); np.random.seed(OUTPUT_SEED)\n",
    "\n",
    "inds_to_map = np.arange(10) #np.random.choice(np.arange((x_inds_test < 100).sum()), size=10, replace=False)\n",
    "number_of_samples = 3\n",
    "\n",
    "mapped_all = []\n",
    "latent_to_map = torch.tensor(test_latents[x_inds_test[inds_to_map]])\n",
    "\n",
    "inp_images = test_images[x_inds_test[inds_to_map]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        latents_n = []\n",
    "        for image in inp_images:\n",
    "            if image.shape[2] == 4:\n",
    "                image= image[:, :, :3]\n",
    "            image = image.transpose((2, 0, 1))\n",
    "    \n",
    "            x = (\n",
    "                torch.tensor(\n",
    "                    np.asarray(image, dtype=np.float32), device=\"cpu\", requires_grad=False\n",
    "                )\n",
    "                / 127.5\n",
    "                - 1.0\n",
    "            )\n",
    "            latents_n.append( model.encode(x[None, ...],8,1)[0].squeeze())\n",
    "        latents_tensor = torch.stack(latents_n) #.detach().cpu().numpy()\n",
    "        mapped = D(latents_tensor) #D(latents_tensor) #D(latent_to_map.cpu())\n",
    "        mapped_all.append(mapped)\n",
    "    \n",
    "mapped = torch.stack(mapped_all, dim=1)\n",
    "\n",
    "decoded_all = []\n",
    "with torch.no_grad():\n",
    "    for k in range(number_of_samples):\n",
    "        decoded_img = decode(model, mapped[:, k])\n",
    "        decoded_img = ((decoded_img * 0.5 + 0.5) * 255).type(torch.long).clamp(0, 255).cpu().type(torch.uint8).permute(0, 2, 3, 1).numpy()\n",
    "        decoded_all.append(decoded_img)\n",
    "        \n",
    "decoded_all = np.stack(decoded_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6521e64f-6210-4271-a2ef-a256d9e1017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, number_of_samples+1, figsize=(number_of_samples+1, 10), dpi=200)\n",
    "\n",
    "for i, ind in enumerate(range(10)):\n",
    "    ax = axes[i]\n",
    "    ax[0].imshow(inp_images[ind])\n",
    "    for k in range(number_of_samples):\n",
    "        ax[k+1].imshow(decoded_all[ind, k])\n",
    "        \n",
    "        ax[k+1].get_xaxis().set_visible(False)\n",
    "        ax[k+1].set_yticks([])\n",
    "        \n",
    "    ax[0].get_xaxis().set_visible(False)\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "fig.tight_layout(pad=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b36e478-fb05-452c-a40d-7b12b94d05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"utk_face_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da920bee-66de-4cc2-8b23-1183d7cfca6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
